{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1 -  Single collection RAG\n",
    "\n",
    "### **SupportPatterns** - Support Training & Education Platform\n",
    "\n",
    "- Develops training materials and courses for customer support professionals\n",
    "- Uses aggregated, anonymized support conversations to create realistic training scenarios\n",
    "\n",
    "### Solution\n",
    "\n",
    "Collect as much conversation data between support agents and customers as possible. \n",
    "\n",
    "Analyse this data to identify common patterns and develop training materials based on these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def download_datafiles(setup: Literal[\"ollama\", \"cohere\"]):\n",
    "    filepaths_set = {\n",
    "        \"ollama\": (\n",
    "            \"https://weaviate-workshops.s3.eu-west-2.amazonaws.com/odsc-europe-2024/twitter_customer_support_weaviate_export_50000_nomic.h5\",\n",
    "            Path(\"data/twitter_customer_support_nomic.h5\")\n",
    "        ),\n",
    "        \"cohere\": (\n",
    "            \"https://weaviate-workshops.s3.eu-west-2.amazonaws.com/odsc-europe-2024/twitter_customer_support_weaviate_export_50000_cohere-embed-multilingual-light-v3.0.h5\",\n",
    "            Path(\"data/twitter_customer_support_cohere.h5\"),\n",
    "        )\n",
    "    }\n",
    "\n",
    "    filepaths = filepaths_set[setup]\n",
    "\n",
    "    if not filepaths[1].exists():\n",
    "        print(f\"Downloading {filepaths[0]}\")\n",
    "        filepaths[1].parent.mkdir(parents=True, exist_ok=True)\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(filepaths[0], filepaths[1])\n",
    "    else:\n",
    "        print(f\"File already exists: {filepaths[1]}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Models\n",
    "\n",
    "This workshop is set up for you to work with local, Ollama models, or API-based Cohere models. Follow either [Ollama](#ollama) or [Cohere](#cohere) instructions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull nomic-embed-text && ollama pull gemma2:2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_datafiles(\"ollama\")\n",
    "\n",
    "model_type = \"ollama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere \n",
    "\n",
    "To use the Cohere API for this workshop, run the below code cell to configure the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_datafiles(\"cohere\")\n",
    "\n",
    "model_type = \"cohere\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create the collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure\n",
    "\n",
    "if model_type == \"ollama\":\n",
    "    vectorizer_config = Configure.NamedVectors.text2vec_ollama(\n",
    "        name=\"text_with_metadata\",\n",
    "        source_properties=[\"text\", \"company_author\"],\n",
    "        vector_index_config=Configure.VectorIndex.hnsw(),\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",\n",
    "        model=\"nomic-embed-text\",\n",
    "    )\n",
    "    generative_config = Configure.Generative.ollama(\n",
    "        api_endpoint=\"http://host.docker.internal:11434\",\n",
    "        model=\"gemma2:2b\"\n",
    "    )\n",
    "else:\n",
    "    vectorizer_config = Configure.NamedVectors.text2vec_cohere(\n",
    "        name=\"text_with_metadata\",\n",
    "        source_properties=[\"text\", \"company_author\"],\n",
    "        vector_index_config=Configure.VectorIndex.hnsw(),\n",
    "        model=\"embed-multilingual-light-v3.0\",\n",
    "    )\n",
    "\n",
    "    generative_config = Configure.Generative.cohere(\n",
    "        model=\"command-r-plus\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.config import Property, DataType, Configure\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    headers={\"X-Cohere-Api-Key\": os.getenv(\"WORKSHOP_COHERE_KEY\")}\n",
    ")\n",
    "\n",
    "collection_name = \"SupportChat\"\n",
    "\n",
    "# For re-running the demo only: Delete existing collection if it exists\n",
    "client.collections.delete(collection_name)\n",
    "\n",
    "# Create a new collection with specified properties and vectorizer configuration\n",
    "chunks = client.collections.create(\n",
    "    name=collection_name,\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT),\n",
    "        # STUDENT TODO:\n",
    "        # Create properties for 'dialogue_id', 'company_author' and 'created_at' - with data types 'int', 'text' and 'date' respectively\n",
    "    ],\n",
    "    vectorizer_config=[vectorizer_config],\n",
    "    generative_config=generative_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_hdf5_obj(file_path):\n",
    "    with h5py.File(file_path, \"r\") as hf:\n",
    "        for uuid in hf.keys():\n",
    "            src_obj = hf[uuid]\n",
    "\n",
    "            # Get the object properties\n",
    "            properties = json.loads(src_obj[\"object\"][()])\n",
    "\n",
    "            # Get the vector(s)\n",
    "            vectors = {}\n",
    "            for key in src_obj.keys():\n",
    "                if key.startswith(\"vector_\"):\n",
    "                    vector_name = key.split(\"_\", 1)[1]\n",
    "                    vectors[vector_name] = np.asarray(src_obj[key])\n",
    "\n",
    "            yield uuid, properties, vectors\n",
    "\n",
    "\n",
    "def get_data_obj(model_type: Literal[\"ollama\", \"cohere\"]):\n",
    "    file_path = Path(\"data/twitter_customer_support_nomic.h5\")\n",
    "    if model_type == \"cohere\":\n",
    "        file_path = Path(\"data/twitter_customer_support_cohere.h5\")\n",
    "\n",
    "    for uuid, properties, vectors in get_hdf5_obj(file_path):\n",
    "        yield uuid, properties, vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with client.batch.fixed_size(batch_size=200) as batch:\n",
    "    for uuid, properties, vectors in tqdm(get_data_obj(model_type)):\n",
    "        batch.add_object(\n",
    "            # STUDENT TODO:\n",
    "            # Define the object to be added - specify the collection name, uuid and properties - the \"vector\" property is pre-defined for you\n",
    "            vector={\"text_with_metadata\": vectors[\"text_with_metadata\"]},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for that our data is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processed {len(client.batch.results.objs.all_responses)} objects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(client.batch.failed_objects) > 0:\n",
    "    print(\"*\" * 80)\n",
    "    print(f\"***** Failed to add {len(client.batch.failed_objects)} objects *****\")\n",
    "    print(\"*\" * 80)\n",
    "    print(client.batch.failed_objects[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve some arbitrary objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a collection object to interact with the collection\n",
    "support_chats = client.collections.get(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT TODO:\n",
    "# Fetch the first two objects from the collection with the vector included\n",
    "# Hint - use the 'query.fetch_objects' method with the 'limit' and 'include_vector' parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT TODO:\n",
    "# Print the UUID of the first object in the response\n",
    "# Hint - The response will have an `.objects` attribute which is a list of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT TODO:\n",
    "# Inspect the properties of the first object in the response\n",
    "# Hint - the object will have a 'properties' attribute which is a dictionary of properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT TODO:\n",
    "# Inspect the first few dimensions of the object's vector\n",
    "# HINT - the object will have a 'vector' attribute which is a dictionary of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for displaying objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_objects(response):\n",
    "    for o in response.objects:\n",
    "        print(o.uuid, \"\\n\")\n",
    "        print(o.properties[\"text\"][:100], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near text search: Semantic search example\n",
    "response = support_chats.query.near_text(\"return process\", limit=3)\n",
    "display_objects(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT TODO:\n",
    "# Run a `bm25` query with the search term \"return process\" and a limit of 3, and display the results\n",
    "# Hint - start with the previous cell, and vary the query method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT TODO:\n",
    "# Run a `hybrid` query with the same parameters and display the results\n",
    "# Hint - start with the previous cell, and vary the query method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative search (RAG) example\n",
    "response = support_chats.generate.fetch_objects(\n",
    "    limit=20,\n",
    "    grouped_task=\"What patterns are we seeing here in these issues?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example use cases\n",
    "\n",
    "- Develop training materials\n",
    "    - Investigate common patterns in support conversations\n",
    "    - Identify common issues and resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How might our example business use these capabilties?\n",
    "# What types of RAG queries would be useful for them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student TODO:\n",
    "# Try your own `grouped_task` query with a different question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource management\n",
    "\n",
    "- How much memory are we using?\n",
    "- How will this scale with more data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use this pattern\n",
    "\n",
    "- Is any of the data isolated from the others?\n",
    "- What use cases might not be covered by this architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Demo application\n",
    "\n",
    "- Outside of the notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
